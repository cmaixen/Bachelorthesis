{
 "metadata": {
  "name": "",
  "signature": "sha256:67495debf1877d6cbbeb9bbf73349485ed8ade8f68f327a9a2dbbf9a0ddb223a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import random\n",
      "np.set_printoptions(threshold=np.nan)\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "import pandas as pd\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import urllib2\n",
      "import pickle\n",
      "import os\n",
      "from sklearn import svm\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn import learning_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from scipy.sparse import csr_matrix\n",
      "import scipy as sp\n",
      "import scipy.stats\n",
      "from nltk.corpus import stopwords\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "import csv\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_file_selection(source, amount):\n",
      "    result = []\n",
      "    limit_random = len(source) - 1\n",
      "    random.randint(0, limit_random)\n",
      "    for i in range(amount):\n",
      "        result = result + [source[random.randint(0, limit_random)]]\n",
      "    return result\n",
      "\n",
      "def retreive_data(source,path,amount):\n",
      "#     print \"amount \" + str(amount)\n",
      "    random_source = random_file_selection(source, amount)\n",
      "    counter = 0\n",
      "    destination = []\n",
      "    for filename in random_source:\n",
      "        with open ( path + filename, \"r\") as myfile:\n",
      "            data = myfile.read().replace('\\n', '')\n",
      "            counter= counter + 1\n",
      "            destination.append(data)\n",
      "            myfile.close()\n",
      "            if amount <= counter:\n",
      "                break\n",
      "    return destination\n",
      "\n",
      "def split_data(pos_reviews, neg_reviews, pos_path, neg_path, total_amount):\n",
      "    subsetamount = total_amount*1/2\n",
      "#     print \"amount subset \" +  str(subsetamount)\n",
      "    pos_data = retreive_data(pos_reviews, pos_path, subsetamount)\n",
      "    neg_data = retreive_data(neg_reviews, neg_path, subsetamount)\n",
      "    cutoff = len(pos_data)*3/4\n",
      "    \n",
      "    traindata = pos_data[:cutoff]  + neg_data[:cutoff]\n",
      "    testdata = pos_data[cutoff:] + neg_data[cutoff:]\n",
      "#     print \"length traindata: \" + str(len(traindata))\n",
      "#     print \"length testdata: \" + str(len(testdata))\n",
      "    return traindata , testdata\n",
      "\n",
      "def merge_data(pos_reviews, neg_reviews, pos_path, neg_path, total_amount):\n",
      "    subsetamount = total_amount*1/2\n",
      "    pos_data = retreive_data(pos_reviews, pos_path, subsetamount)\n",
      "    neg_data = retreive_data(neg_reviews, neg_path, subsetamount)\n",
      "    return pos_data + neg_data\n",
      "\n",
      "def pre_proces(pos_directoryindex,pos_path,neg_directoryindex,neg_path,limit):\n",
      "    traindatas = []\n",
      "    testdatas = []\n",
      "\n",
      "    for i in range(amount_for_average):\n",
      "#         print i\n",
      "#         print \"limit\" + str(limit)\n",
      "        traindata, testdata =  split_data(pos_directoryindex,neg_directoryindex,pos_path,neg_path,limit)\n",
      "\n",
      "        traindatas.append(traindata)\n",
      "        testdatas.append(testdata)\n",
      "        \n",
      "    return [traindatas , testdatas]\n",
      "\n",
      "def get_data(pos_path, neg_path, limit):\n",
      "\n",
      "    pos_directoryindex = os.listdir(pos_path)[1:len(os.listdir(pos_path))]\n",
      "    neg_directoryindex = os.listdir(neg_path)[1:len(os.listdir(neg_path))]\n",
      "    return pre_proces(pos_directoryindex,pos_path,neg_directoryindex,neg_path,limit)\n",
      "\n",
      "def mean_confidence_interval(data, confidence=0.95):\n",
      "    a = 1.0*np.array(data)\n",
      "    n = len(a)\n",
      "    m, se = np.mean(a), scipy.stats.sem(a)\n",
      "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
      "    return m, m-h, m+h\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_results(filename, title, train_res , test_res):\n",
      "    with open(filename, 'a') as file_:\n",
      "        fieldnames = ['Title','Precisie_Trainingsset', 'Precisie_Testset']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
      "        writer.writeheader()\n",
      "        writer.writerow({'Title': title, 'Precisie_Trainingsset': train_res, 'Precisie_Testset': test_res})\n",
      "    file_.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Naive Bayes Classifier - Bag of Words / TFIDF / Bigram"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Het vectorizer argument bepaald wat je meegeeft. \n",
      "# Bag of words = CountVectorizer\n",
      "# Verwijderen van stopwoorden = TFIDFVectorizer\n",
      "\n",
      "def evaluate_NB_with_average(traindatas,testdatas ,Vectorizer, title, filename, amount=amount_for_average):\n",
      "    print 'Takes average of' + str(amount) + 'runs'\n",
      "    \n",
      "    def evaluate_classifier(traindata, testdata):\n",
      "        \n",
      "        trainvectorizer = Vectorizer\n",
      "        trainset = trainvectorizer.fit_transform(traindata)\n",
      "        \n",
      "        cutoff = trainset.shape[0]*1/2\n",
      "        X_train = trainset\n",
      "#         print cutoff\n",
      "        y_train = np.concatenate([np.asarray([\"pos\"] * cutoff) , np.asarray([\"neg\"] * cutoff)], axis=1)\n",
      "        \n",
      "     \n",
      "        X_test = trainvectorizer.transform(testdata)\n",
      "        cutoff = X_test.shape[0]*1/2\n",
      "#         print cutoff\n",
      "        y_test = np.concatenate([np.asarray([\"pos\"] * cutoff) , np.asarray([\"neg\"] * cutoff)], axis=1)\n",
      "\n",
      "#         print 'train on %d instances, test on %d instances' % (len(y_train), len(y_test))\n",
      "        \n",
      "        \n",
      "        classifier = MultinomialNB().fit(X_train,y_train)\n",
      "        \n",
      "        score_test = classifier.score(X_test,y_test)\n",
      "        score_train = classifier.score(X_train,y_train)\n",
      "        y_pred = classifier.predict(X_test)\n",
      "#         print \"Accuracy Naive Bayes with Trainingsset : \" + str(score_train)\n",
      "#         print \"Accuracy Naive Bayes with Testset : \" + str(score_test)\n",
      "        cm = confusion_matrix(y_test,y_pred)\n",
      "#         print \"confusion matrix: \"\n",
      "#         print cm\n",
      "        \n",
      "        return [score_train,score_test, classifier, cm , trainvectorizer]\n",
      "        \n",
      "    score_testset = []\n",
      "    score_trainset = []\n",
      "    lastmodel =  []\n",
      "    trainvectorizers = []\n",
      "    average_cm = 0\n",
      "    for i in range(amount):\n",
      "#             print 'Run ', i+1, 'started...'\n",
      "            try: \n",
      "                result = evaluate_classifier(traindatas[i], testdatas[i])\n",
      "                score_trainset.append(result[0])\n",
      "                score_testset.append(result[1])\n",
      "                lastmodel.append(result[2])\n",
      "                average_cm = average_cm  + result[3]\n",
      "                trainvectorizers.append(result[4])\n",
      "            except ZeroDivisionError:\n",
      "                print \"Oops there was a division by zero\"\n",
      "#             print 'Done'\n",
      "            \n",
      "    print \"-------------------------------------\"\n",
      "#     print score_testset\n",
      "    print \"Standard Deviation\" + str(np.std(score_testset))\n",
      "    print average_cm\n",
      "    print \"Mean Confidence Interval: \" + str(mean_confidence_interval(score_testset))\n",
      "    print \"Average Accuracy of Naive Bayes with Trainingsset : \" + str(np.mean(score_trainset))\n",
      "    print \"Average Accuracy of Naive Bayes with Testset : \" + str(np.mean(score_testset))\n",
      "    print \"Average Confusion Matrix with Testset : \" + str(average_cm / amount_for_average)\n",
      "    write_results(filename, title, str(np.mean(score_trainset)),str(np.mean(score_testset)))\n",
      "    return [lastmodel , trainvectorizers]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def evaluate_NB_with_average_LSA(traindatas,testdatas ,Vectorizer, n_features , title, filename, amount=amount_for_average):\n",
      "    print 'Takes average of' + str(amount) + 'runs'\n",
      "    \n",
      "    def evaluate_classifier(traindata, testdata):\n",
      "        \n",
      "        trainvectorizer = Vectorizer\n",
      "        trainset = trainvectorizer.fit_transform(traindata)\n",
      "        \n",
      "        svd = TruncatedSVD(n_components = n_features)\n",
      "        #matrix opstellen en reduceren\n",
      "        trainset = svd.fit_transform(trainset)\n",
      "       \n",
      "        cutoff = trainset.shape[0]*1/2\n",
      "        X_train = trainset\n",
      "#         print cutoff\n",
      "        y_train = np.concatenate([np.asarray([\"pos\"] * cutoff) , np.asarray([\"neg\"] * cutoff)], axis=1)\n",
      "        \n",
      "     \n",
      "       #testdata ook reduceren\n",
      "        X_test = svd.transform(trainvectorizer.transform(testdata))\n",
      "        cutoff = X_test.shape[0]*1/2\n",
      "#         print cutoff\n",
      "        y_test = np.concatenate([np.asarray([\"pos\"] * cutoff) , np.asarray([\"neg\"] * cutoff)], axis=1)\n",
      "\n",
      "#         print 'train on %d instances, test on %d instances' % (len(y_train), len(y_test))\n",
      "        \n",
      "        \n",
      "        classifier = GaussianNB().fit(X_train,y_train)\n",
      "        \n",
      "        score_test = classifier.score(X_test,y_test)\n",
      "        score_train = classifier.score(X_train,y_train)\n",
      "        y_pred = classifier.predict(X_test)\n",
      "#         print \"Accuracy Naive Bayes with Trainingsset : \" + str(score_train)\n",
      "#         print \"Accuracy Naive Bayes with Testset : \" + str(score_test)\n",
      "        cm = confusion_matrix(y_test,y_pred)\n",
      "#         print \"confusion matrix: \"\n",
      "#         print cm\n",
      "        \n",
      "        return [score_train,score_test, classifier, cm , trainvectorizer]\n",
      "        \n",
      "    score_testset = []\n",
      "    score_trainset = []\n",
      "    lastmodel =  []\n",
      "    trainvectorizers = []\n",
      "    average_cm = 0\n",
      "    for i in range(amount):\n",
      "#             print 'Run ', i+1, 'started...'\n",
      "            try: \n",
      "                result = evaluate_classifier(traindatas[i], testdatas[i])\n",
      "                score_trainset.append(result[0])\n",
      "                score_testset.append(result[1])\n",
      "                lastmodel.append(result[2])\n",
      "                average_cm = average_cm  + result[3]\n",
      "                trainvectorizers.append(result[4])\n",
      "            except ZeroDivisionError:\n",
      "                print \"Oops there was a division by zero\"\n",
      "#             print 'Done'\n",
      "            \n",
      "    print \"-------------------------------------\"\n",
      "#     print score_testset\n",
      "    print \"Standard Deviation\" + str(np.std(score_testset))\n",
      "    print average_cm\n",
      "    print \"Mean Confidence Interval: \" + str(mean_confidence_interval(score_testset))\n",
      "    print \"Average Accuracy of Naive Bayes with Trainingsset : \" + str(np.mean(score_trainset))\n",
      "    print \"Average Accuracy of Naive Bayes with Testset : \" + str(np.mean(score_testset))\n",
      "    print \"Average Confusion Matrix with Testset : \" + str(average_cm / amount_for_average)\n",
      "    write_results(filename, title, str(np.mean(score_trainset)),str(np.mean(score_testset)))\n",
      "    return [lastmodel , trainvectorizers]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def evaluate_NB_with_average_BF(traindatas,testdatas ,Vectorizer, k_features , title, filename, amount=amount_for_average):\n",
      "    print 'Takes average of' + str(amount) + 'runs'\n",
      "    \n",
      "    def evaluate_classifier(traindata, testdata):\n",
      "        \n",
      "        trainvectorizer = Vectorizer\n",
      "        trainset = trainvectorizer.fit_transform(traindata)\n",
      "        \n",
      "        KB = SelectKBest(chi2, k=k_features)\n",
      "        #matrix opstellen en reduceren\n",
      "       \n",
      "        cutoff = trainset.shape[0]*1/2\n",
      "        X_train = trainset\n",
      "#         print cutoff\n",
      "        y_train = np.concatenate([np.asarray([\"pos\"] * cutoff) , np.asarray([\"neg\"] * cutoff)], axis=1)\n",
      "        \n",
      "        X_train = KB.fit_transform(X_train,y_train)\n",
      "     \n",
      "       #testdata ook reduceren\n",
      "        X_test = KB.transform(trainvectorizer.transform(testdata))\n",
      "        cutoff = X_test.shape[0]*1/2\n",
      "#         print cutoff\n",
      "        y_test = np.concatenate([np.asarray([\"pos\"] * cutoff) , np.asarray([\"neg\"] * cutoff)], axis=1)\n",
      "\n",
      "#         print 'train on %d instances, test on %d instances' % (len(y_train), len(y_test))\n",
      "        \n",
      "        \n",
      "        classifier = GaussianNB().fit(X_train,y_train)\n",
      "        \n",
      "        score_test = classifier.score(X_test,y_test)\n",
      "        score_train = classifier.score(X_train,y_train)\n",
      "        y_pred = classifier.predict(X_test)\n",
      "#         print \"Accuracy Naive Bayes with Trainingsset : \" + str(score_train)\n",
      "#         print \"Accuracy Naive Bayes with Testset : \" + str(score_test)\n",
      "        cm = confusion_matrix(y_test,y_pred)\n",
      "#         print \"confusion matrix: \"\n",
      "#         print cm\n",
      "        \n",
      "        return [score_train,score_test, classifier, cm , trainvectorizer]\n",
      "        \n",
      "    score_testset = []\n",
      "    score_trainset = []\n",
      "    lastmodel =  []\n",
      "    trainvectorizers = []\n",
      "    average_cm = 0\n",
      "    for i in range(amount):\n",
      "#             print 'Run ', i+1, 'started...'\n",
      "            try: \n",
      "                result = evaluate_classifier(traindatas[i], testdatas[i])\n",
      "                score_trainset.append(result[0])\n",
      "                score_testset.append(result[1])\n",
      "                lastmodel.append(result[2])\n",
      "                average_cm = average_cm  + result[3]\n",
      "                trainvectorizers.append(result[4])\n",
      "            except ZeroDivisionError:\n",
      "                print \"Oops there was a division by zero\"\n",
      "#             print 'Done'\n",
      "            \n",
      "    print \"-------------------------------------\"\n",
      "#     print score_testset\n",
      "    print \"Standard Deviation\" + str(np.std(score_testset))\n",
      "    print average_cm\n",
      "    print \"Mean Confidence Interval: \" + str(mean_confidence_interval(score_testset))\n",
      "    print \"Average Accuracy of Naive Bayes with Trainingsset : \" + str(np.mean(score_trainset))\n",
      "    print \"Average Accuracy of Naive Bayes with Testset : \" + str(np.mean(score_testset))\n",
      "    print \"Average Confusion Matrix with Testset : \" + str(average_cm / amount_for_average)\n",
      "    write_results(filename, title, str(np.mean(score_trainset)),str(np.mean(score_testset)))\n",
      "    return [lastmodel , trainvectorizers]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 183
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variablen"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "limit_scanning = 8000\n",
      "amount_for_average = 30\n",
      "dutch_stopwords = stopwords.words('dutch')\n",
      "english_stopwords = stopwords.words('english')\n",
      "eng_results = 'english_results.csv'\n",
      "nl_results = 'dutch_results.csv'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 184
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Engelse Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_path_eng = \"/Users/yannickmerckx/MEGAsync/Unif/3de Bachelor/Bachelorproef/Bachelorproef/english_dataset/train/pos/\"\n",
      "neg_path_eng = \"/Users/yannickmerckx/MEGAsync/Unif/3de Bachelor/Bachelorproef/Bachelorproef/english_dataset/train/neg/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 185
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_eng  = get_data(pos_path_eng,neg_path_eng,limit_scanning)\n",
      "traindata_eng = data_eng[0]\n",
      "testdata_eng = data_eng[1]\n",
      "print \"length traindata: \" + str(len(traindata_eng[1]))\n",
      "print \"length testdata: \" + str(len(testdata_eng[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "length traindata: 6000\n",
        "length testdata: 2000\n"
       ]
      }
     ],
     "prompt_number": 186
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Nederlandse Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_path_nl = \"/Users/yannickmerckx/MEGAsync/Unif/3de Bachelor/Bachelorproef/Bachelorproef/moviemeter/pos/\"\n",
      "neg_path_nl = \"/Users/yannickmerckx/MEGAsync/Unif/3de Bachelor/Bachelorproef/Bachelorproef/moviemeter/neg/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_nl  = get_data(pos_path_nl,neg_path_nl,limit_scanning)\n",
      "traindata_nl = data_nl[0]\n",
      "testdata_nl = data_nl[1]\n",
      "print \"length traindata: \" + str(len(traindata_nl[1]))\n",
      "print \"length testdata: \" + str(len(testdata_nl[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "length traindata: 6000\n",
        "length testdata: 2000\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Als eerste hypothese denk ik dat er meer stopwoorden zijn in het Nederlands en dat het relevant is hier aandacht aan te besteden. Verdere combinaties zijn nog mogelijk maar we zullen deze resultaten eerst is bekijken.\n",
      "\n",
      "De dataset bestaat telkens uit 1/2 uit positief en 1/2 negatief en wordt random samengesteld. Zodanig dat de classificatie at random 50% is.\n",
      "\n",
      "De classificatie wordt telkens 30 keer opnieuw met een andere dataset herhaald. Om zo statistisch intervallen te kunnen opstellen. Aangezien we een normaal verdeling hebben kunnen we dit.\n",
      "\n",
      "Trainingset en testset samen bestaan uit uit 8000 samples"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bag of Words (geen preprocessing)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, CountVectorizer(), 'Bag of Words' , eng_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, CountVectorizer(), 'Bag of Words' , nl_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verwijderen van Stopwoorden"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, CountVectorizer(stop_words=english_stopwords), 'Verwijderen van stopwoorden' , eng_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, CountVectorizer(stop_words=dutch_stopwords), 'Verwijderen van stopwoorden' , nl_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Term weighting "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, TfidfVectorizer() , 'Term Weighting' , eng_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, TfidfVectorizer(), 'Term Weighting' , nl_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bigram Collocaties"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, CountVectorizer(ngram_range=(1, 2)), 'Bigram Collocaties' , eng_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, CountVectorizer(ngram_range=(1, 2)), 'Bigram Collocaties' , nl_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "LSA"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "LSA - on Bag of Words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, CountVectorizer(), 2, 'LSA on Bag of Words' , eng_results))\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, CountVectorizer(), 100, 'LSA on Bag of Words' , eng_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, CountVectorizer(), 2, 'LSA on Bag of Words' , nl_results)\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, CountVectorizer(), 100, 'LSA on Bag of Words' , nl_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "LSA - on TFIDF"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, TfidfVectorizer(), 2, 'LSA on TFIDF' , eng_results)\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, TfidfVectorizer(), 100, 'LSA on TFIDF' , eng_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, TfidfVectorizer(), 2, 'LSA on Bag of Words' , nl_results)\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, TfidfVectorizer(), 100, 'LSA on Bag of Words' , nl_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Best feature selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K moeten we bepalen op basis van een elbow test"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Best feature selection - on Bag of Words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, CountVectorizer(), 10, 'Best Feature selection on Bag of Words' , eng_results )\n",
      "# print \"Best 30 features\"\n",
      "# NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng,CountVectorizer(), 30, 'Best Feature selection on Bag of Words' , eng_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, CountVectorizer(), 10 , 'Best Feature selection on Bag of Words' , nl_results  )\n",
      "print \"Best 30 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, CountVectorizer(), 30, 'Best Feature selection Bag of Words' , nl_results  )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Best feature selection - on TFIDF"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, TfidfVectorizer(), 10 , 'Best Feature selection on TFIDF' , eng_results  )\n",
      "print \"Best 30 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng,TfidfVectorizer(), 30,  'Best Feature selection on TFIDF' , eng_results  )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, TfidfVectorizer(), 10, 'Best Feature selection on TFIDF' , nl_results  )\n",
      "print \"Best 30 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl,TfidfVectorizer(), 30, 'Best Feature selection on TFIDF' , nl_results  )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verwijderen van stopwoorden + Term Weighting"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, TfidfVectorizer(stop_words=english_stopwords), 'Verwijderen van stopwoorden + Term Weighting' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, TfidfVectorizer(stop_words=dutch_stopwords),'Verwijderen van stopwoorden + Term Weighting' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verwijderen van stopwoorden + Bigram Collocaties"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, CountVectorizer(stop_words=english_stopwords, ngram_range=(1, 2)),'Verwijderen van stopwoorden + Bigram Collocaties' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, CountVectorizer(stop_words=dutch_stopwords,ngram_range=(1, 2)),'Verwijderen van stopwoorden + Bigram Collocaties' , nl_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verwijderen van stopwoorden + LSA"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "LSA - on Bag of Words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, CountVectorizer(stop_words=english_stopwords), 2, 'Verwijderen van stopwoorden + LSA on Bag of Words' , eng_results )\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, CountVectorizer(stop_words=english_stopwords), 100, 'Verwijderen van stopwoorden + LSA on Bag of Words' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, CountVectorizer(stop_words=dutch_stopwords), 2,'Verwijderen van stopwoorden + LSA on Bag of Words' , nl_results )\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, CountVectorizer(stop_words=dutch_stopwords), 100,'Verwijderen van stopwoorden + LSA on Bag of Words' , nl_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "LSA - on TFIDF"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, TfidfVectorizer(stop_words=english_stopwords), 2, 'Verwijderen van stopwoorden + LSA on TFIDF' , eng_results )\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_eng, testdata_eng, TfidfVectorizer(stop_words=english_stopwords), 100, 'Verwijderen van stopwoorden + LSA on TFIDF' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"2 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, TfidfVectorizer(stop_words=dutch_stopwords), 2,'Verwijderen van stopwoorden + LSA on TFIDF' , nl_results )\n",
      "print \"100 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average_LSA(traindata_nl, testdata_nl, TfidfVectorizer(stop_words=dutch_stopwords), 100, 'Verwijderen van stopwoorden + LSA on TFIDF' , nl_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verwijderen van stopwoorden + Bigram Collocaties + Term Weighting"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, TfidfVectorizer(stop_words=dutch_stopwords,ngram_range=(1, 2)), 'Verwijderen van stopwoorden + Bigram Collocaties + Term Weighting' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, TfidfVectorizer(stop_words=dutch_stopwords,ngram_range=(1, 2)),'Verwijderen van stopwoorden + Bigram Collocaties + Term Weighting' , nl_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verwijderen van stopwoorden + Best feature selection"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Best feature selection - on Bag of Words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, CountVectorizer(stop_words=english_stopwords), 10, 'Verwijderen van stopwoorden + Best feature selection on Bag of Words' , eng_results )\n",
      "print \"Best 30 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng,CountVectorizer(stop_words=english_stopwords), 30, 'Verwijderen van stopwoorden + Best feature selection on Bag of Words' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, CountVectorizer(stop_words=dutch_stopwords), 10, 'Verwijderen van stopwoorden + Best feature selection on Bag of Words' , nl_results )\n",
      "print \"Best 30 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl,CountVectorizer(stop_words=dutch_stopwords), 30, 'Verwijderen van stopwoorden + Best feature selection on Bag of Words' , nl_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Best feature selection - on TFIDF"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ENG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng, TfidfVectorizer(stop_words=english_stopwords), 10, 'Verwijderen van stopwoorden + Best feature selection on TFIDF' , eng_results )\n",
      "print \"Best 30 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_eng, testdata_eng,TfidfVectorizer(stop_words=english_stopwords), 30, 'Verwijderen van stopwoorden + Best feature selection on TFIDF' , eng_results )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "NL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Best 10 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl, TfidfVectorizer(stop_words=dutch_stopwords), 10,'Verwijderen van stopwoorden + Best feature selection on TFIDF' , nl_results )\n",
      "print \"Best 30 features\"\n",
      "NB , NB_vectorizers = evaluate_NB_with_average(traindata_nl, testdata_nl,TfidfVectorizer(stop_words=dutch_stopwords), 30, 'Verwijderen van stopwoorden + Best feature selection on TFIDF' , nl_results ) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}